Setup a single Apache Kafka node on CentOS 7

1)[kafka@kafka1 ~]$ wget http://apache.forsale.plus/kafka/1.0.0/kafka_2.11-1.0.0.tgz
Let's decompress the tar.gz file

[kafka@kafka1 ~]$ tar -xzf kafka_2.11-1.0.0.tgz
That's it! As the Kafka archive already includes ZooKeeper we already have a fully working Kafka installation on our system. Let's adjust the configuration.

2)Create directories
We have to create data and log directories for ZooKeeper and Kafka. To simplify this process we can add the directories within the user home directory. In a merchantion environment, we would use different locations, e.g. separate mount points or physical disks for data and log directories.

[kafka@kafka1 ~]$ mkdir -p /home/kafka/zookeeper/data
[kafka@kafka1 ~]$ mkdir -p /home/kafka/kafka/kafka-logs

3)ZooKeeper configuration
The configuration file of the embedded ZooKeeper instance is located at kafka_2.11-1.0.0/config/zookeeper.properties.

[kafka@kafka1 ~]$ vi kafka_2.11-1.0.0/config/zookeeper.properties
Within this file, we have to locate the dataDir property and set the value to point to the new ZooKeeper directory we created above.

dataDir=/home/kafka/zookeeper/data
Each Kafka node needs a unique server id. ZooKeeper looks up this information from the following file: /home/kafka/zookeeper/data/myid. As we have only one node we can simply assign the value "1" for our instance.

[kafka@kafka1 ~]$ echo "1" > /home/kafka/zookeeper/data/myid
Apache Kafka configuration
Now we can adjust the Kafka configuration files stored here: kafka_2.11-1.0.0/config/server.properties.

[kafka@kafka1 ~]$ vi kafka_2.11-1.0.0/config/server.properties
Similar to the ZooKeeper configuration, each Kafka node needs a unique id. We have to find the broker.id property in the configuration file and change the id. I recommend using the same value as we used for ZooKeeper: 1.

broker.id=1
We also have to change the log directory location specified in the log.dirs parameter.

log.dirs=/home/kafka/kafka/kafka-logs
Additionally, we have to update the listeners and advertised.listeners properties with the Kafka node IP address - in my example 192.168.1.120. We can look up the IP address with ip a.

listeners: the address / server name and protocol kafka is listening to (internal traffic between Kafka nodes)
advertised.listener: the address / server name and protocol clients can use to connect to the Kafka cluster (external traffic). Only need to be specified if different from above setting.
listeners=PLAINTEXT://192.168.1.120:9092
advertised.listeners=PLAINTEXT://192.168.1.120:9092
In a development environment, I usually add the property delete.topic.enable. Setting this property to true allows us to easily delete topics at runtime. If this property is not being set, Kafka will only mark topics as deleted.

delete.topic.enable=true
That's it, we have configured our single node Kafka server!

Start and test the cluster setup
We finally can startup ZooKeeper and Kafka and perform a quick test.

Start ZooKeeper
To start ZooKeeper we execute the following command:


NOTE
nitish.saxena001100@gmail.com
installed dir: /cygdrive/c/services/kafka/kafka


netstat -ano | findstr 2181
taskkill /F /pid 3060
nohup bin/zookeeper-server-start.sh config/zookeeper.properties &
nohup bin/kafka-server-start.sh config/server.properties &

kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 6 --topic PRODUCT_CREATE
kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 6 --topic PRODUCT_UPDATE
kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 6 --topic PRODUCT_DELETE
kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 6 --topic PRODUCT_PATCH
kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 6 --topic PRODUCT_GET
kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 6 --topic PRODUCT_GETALL

Create a new topic
To test the setup we have to create a topic.

6)[kafka@kafka1 ~]$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 6 --topic topic1 --config cleanup.policy=delete --config delete.retention.ms=60000
The command above creates a new topic named topic1 with 6 partitions.

7)We can also get a list of all existing topics

[kafka@kafka1 ~]$ bin/kafka-topics.sh --list --zookeeper loaclhost:2181
And we can get a detailed description of our topic.

[kafka@kafka1 ~]$ bin/kafka-topics.sh --describe --zookeeper kafka:2181 --topic topic1
In my case the command above prints out:

Topic:topic1	PartitionCount:6	ReplicationFactor:1	Configs:delete.retention.ms=60000,cleanup.policy=delete
	Topic: topic1	Partition: 0	Leader: 1	Replicas: 1	Isr: 1
	Topic: topic1	Partition: 1	Leader: 1	Replicas: 1	Isr: 1
	Topic: topic1	Partition: 2	Leader: 1	Replicas: 1	Isr: 1
	Topic: topic1	Partition: 3	Leader: 1	Replicas: 1	Isr: 1
	Topic: topic1	Partition: 4	Leader: 1	Replicas: 1	Isr: 1
	Topic: topic1	Partition: 5	Leader: 1	Replicas: 1	Isr: 1
The command shows which server is responsible for which partition and which server replicates the data. In our case, with only one node, node 1 stores all data.

11)Test the cluster
The Kafka package already includes two command line tools to create a producer and a consumer that can be used to check if the node works.

We can start the producer on our Kafka server using the command below. The command opens a prompt and anything we enter here will be sent to the topic.

[kafka@kafka1 ~]$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topic1
Now we can start a consumer on our Kafka server.

[kafka@kafka1 ~]$ bin/kafka-console-consumer.sh --bootstrap-server loaclhost:9092 --topic topic1
Whenever we enter something in the producer prompt it will be printed out in our consumer terminal.

This means our test was successful, the Kafka instance has been set up